---
title: "Gender Prediction by Voice - 2820L Term Project"
author: "Alex DiCesare, Kenneth Li, and Ken Yang"
date: "April 22, 2017"
output: 
  html_document:
    toc: true
    toc_float: true

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



# Gender Recognition by Voice

Basic environment setup code.
```{r, results='hide'} 
library(ggplot2)
library(dplyr, warn.conflicts = FALSE)
library(broom)
library(class)
library(caret)
library(plotROC)
voice <- read.csv("voice.csv")
## summary(voice)
```

## Introduction 

## Data Exploration

## Models

### Logistic Regression
Among all the models we tried on this data set, the first we consider is regression. As stated in Data Exploration section, our label is a binomial variable, linear regression is not a candidate.
Therefore, we try logistic regression. 

#### Basic Logistic
Our data set has 20 numerical features. At the first glance with a density plot in previous section, we estimate that mean fundamental frequency (meanfun), IQR and first frequency quantile (Q25) are the most dominant variables. Thus, we build our basic logistic model on these 3 variables.

```{r}
# Logistic Regression using meanfun, IQR, Q25
fit.mf <- glm(
  label~meanfun+IQR+Q25, 
  data = voice, 
  family = "binomial"
)
# get the prediction and mutate it into binary output
mf.prob.pred <- predict(
  fit.mf, 
  newdata = voice, 
  type = "response"
)

mf.outcome.pred <- ifelse(mf.prob.pred > .5, 
                          "predict male", 
                          "predict female")

mf.df.prediction <- data.frame(
  predict = mf.outcome.pred,
  actual = voice$label
)

# get the prediction table (updated for meanfun model)
mf.table <- table(mf.df.prediction$predict, mf.df.prediction$actual)
mf.table
```

As the result shows, the model gives a true positive rate of (assume female to be true) 1524 / 1584 = 0.96, which is a very nice result. 

We have also plot the ROC plot of our basic model.
```{r}

```


#### Improve Logistic
Although the tp rate of our basic model is quite satisfying, we can't prove that our model uses the best variable set. One way to improve our model is to use the variable selection technique we learn in homework. 

```{r, results = "hide"}
## predict gender using variable selection
fit.all <- glm(label~., data = voice, family="binomial")
fit.none <- glm(label~1, data = voice, family="binomial")

# Forward selection
fit.result <- step(fit.none, 
                   scope = list(lower = fit.none,
                                upper = fit.all),
                   direction = "forward")

all.prob.pred <- predict(fit.result, newdata = voice, type = "response")
all.outcome.pred <- ifelse(all.prob.pred > .5, 
                           "predict male", 
                           "predict female")

all.df.prediction <- data.frame(
  predict = all.outcome.pred,
  actual = voice$label
)

all.table <- table(all.df.prediction$predict, all.df.prediction$actual)
```

```{r}
fit.result
all.table
```
As we can see, using variable selection, the final model is based on 8 variables out of the 20. The tp rate in this model is 97.3%, higher than our basic model.

roc plot...

### Supervised Machine Learning 




## Results and Conclusion







